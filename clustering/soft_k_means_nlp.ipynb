{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP Tasks** -- \n",
    "\n",
    "1. Read the file and store the list of titles\n",
    "2. Remove the stopwords and perform stemming\n",
    "3. Create the word to index\n",
    "4. Create the input matrix i.e. vocabulary * document data frame\n",
    "5. Create the tf_idf or the count_vectorizer\n",
    "6. Run K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2373"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = [line.rstrip() for line in open(\"data/books_titles/all_book_titles.txt\")]\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(w.rstrip() for w in open('data/books_titles/stopwords.txt'))\n",
    "\n",
    "# Adding few more stopwords\n",
    "stopwords = stopwords.union({\n",
    "    'introduction', 'edition', 'series', 'application',\n",
    "    'approach', 'card', 'access', 'package', 'plus', 'etext',\n",
    "    'brief', 'vol', 'fundamental', 'guide', 'essential', 'printed',\n",
    "    'third', 'second', 'fourth', })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "\n",
    "def my_tokenizer(book_title):\n",
    "    book_title = book_title.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(book_title)\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    tokens = [t for t in tokens if not any(c.isdigit() for c in t)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word to index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word-to-index map so that we can create our word-frequency vectors later.\n",
    "\n",
    "\n",
    "'''\n",
    "Pseudo-code:\n",
    "# For each token in the list of tokens\n",
    "    Check if the token is in the dictionary keys.\n",
    "    If no, add the token key into the dictionary and corresponding count in the value\n",
    "    else, increment the counter for that key.\n",
    "'''\n",
    "\n",
    "word_index_map = {}  # word: index dictionary\n",
    "current_index = 0    # Index starts from 0 \n",
    "\n",
    "# let's also save the tokenized versions so we don't have to tokenize again later.\n",
    "all_tokens = []      # all the words in each title, includes repeated words\n",
    "\n",
    "all_titles = []      # Number of documents\n",
    "index_word_map = []  # List of unique words\n",
    "\n",
    "for title in titles:\n",
    "    #print(title)\n",
    "    #try:\n",
    "    #title = title.encode('ascii', 'ignore') # this will throw exception if bad characters\n",
    "    all_titles.append(title)\n",
    "    tokens = my_tokenizer(title)\n",
    "    all_tokens.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "            index_word_map.append(token)\n",
    "#    except:\n",
    "#        print(\"Exception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Alternate solution **\n",
    "\n",
    "The idea is to give the lower index values for high frequency words.\n",
    "\n",
    "Why? Then you could consider only the top 5000 words as dimension instead of using all the words. Since the top 5000 words are \n",
    "highly occuring words, it would be safe to take them rather than the previous approach.\n",
    "\n",
    "Sorting and creating index based on higher values.\n",
    "Creating word_index_mapping dictionary with the word as the key and value as the index, such that the most frequent word has the \n",
    "lower index value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''word_dict = {}\n",
    "\n",
    "for t in tokens:\n",
    "    if t in word_dict.keys():\n",
    "        word_dict[t] += 1\n",
    "    else: \n",
    "        word_dict[t] = 1\n",
    "\n",
    "word_series = pd.Series(word_dict).sort_values(ascending=False)\n",
    "\n",
    "word_index = {}\n",
    "for i,j in enumerate(word_series.index):\n",
    "    word_index[j] = i\n",
    "\n",
    "'''    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the input matrix with the count_values as the cell values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ef546b4dc385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_index_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# terms will go along rows, documents along columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# Now let's create our input matrices - just indicator variables\n",
    "# Implementing the count_vectorizer\n",
    "\n",
    "def tokens_to_vector(tokens):\n",
    "    x = np.zeros(len(word_index_map))   # word_index_map - {word:index}  \n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    return x\n",
    "\n",
    "# NOTE: we are creating an array of D*N dimension i.e. words in the rows and documents in the columns.\n",
    "\n",
    "N = len(all_titles)      # Number of documents\n",
    "D = len(word_index_map)  # Number of unique words\n",
    "X = np.zeros((D, N)) # terms will go along rows, documents along columns\n",
    "i = 0\n",
    "\n",
    "for tokens in all_tokens:\n",
    "    X[:,i] = tokens_to_vector(tokens)   # Filling the title column\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_calculation(input_array, centroid):\n",
    "    \n",
    "    diff = input_array - centroid\n",
    "    sqr_dist = np.power(diff, 2)    \n",
    "    sum_sqr_dist = np.sum(sqr_dist, axis=1)\n",
    "    \n",
    "    return sum_sqr_dist \n",
    "    #np.exp(-1 * np.sum(np.power((input_array -  centroids[k,:]), 2), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(resp, input_array, centroid):\n",
    "    cost = 0\n",
    "# =============================================================================\n",
    "#     for k in range(len(centroid)):\n",
    "#         for n in range(len(input_array)):\n",
    "#             cost += resp[n,k] * dist(centroid[k], input_array[n])\n",
    "# =============================================================================\n",
    "\n",
    "    for k in range(len(centroid)):        \n",
    "        cost += resp[:,k].dot(dist_calculation(input_array, centroid[k]))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_means(input_array, \n",
    "                 n_clusters, \n",
    "                 beta=1,\n",
    "                 max_iter=20, \n",
    "                 show_plots=False):\n",
    "    \n",
    "    nrow = len(input_array)\n",
    "    \n",
    "    np.random.seed(100)\n",
    "    init_centroid_index = np.random.randint(0, nrow, n_clusters)\n",
    "    centroids = input_array[init_centroid_index]\n",
    "\n",
    "    exponents = np.empty((nrow, n_clusters))\n",
    "    costs = np.zeros(max_iter)\n",
    "    \n",
    "    last_iter_num = 0\n",
    "    \n",
    "    for iter_num in range(max_iter):        \n",
    "     \n",
    "        # Step 1: Calculating the responsibility\n",
    "        \n",
    "# =============================================================================\n",
    "#         for k in range(n_clusters):\n",
    "#             for n in range(nrow):\n",
    "#                 exponents[n,k] = np.exp(-beta * dist(input_array[n,:], centroids[k,:]))\n",
    "# =============================================================================\n",
    "    \n",
    "        # Vectorization       \n",
    "        for k in range(n_clusters):          \n",
    "            exponents[:, k] = np.exp(-1 * dist_calculation(input_array, centroids[k,:]))\n",
    "            #exponents[:, k] = np.exp(-beta * np.sum(np.power((input_array - centroids[k,:]), 2), axis=1))         \n",
    "            \n",
    "        resp = exponents / np.sum(exponents, axis=1, keepdims=True)\n",
    "        \n",
    "        # Step 2: Recalculate the means\n",
    "        for k in range(n_clusters):\n",
    "            centroids[k] = resp[:, k].dot(input_array) / np.sum(resp[:, k])\n",
    "        \n",
    "        costs[iter_num] = cost(resp, input_array, centroids)\n",
    "        \n",
    "        if iter_num > 0:\n",
    "            if np.abs(costs[iter_num] - costs[iter_num -1]) < 10e-5:\n",
    "                print(\"converged at iteration number {0:d}\".format(iter_num))\n",
    "                last_iter_num = iter_num\n",
    "                break;\n",
    "    \n",
    "    #cluster_assignment = resp.argmax(axis=1)\n",
    "        \n",
    "    if show_plots:\n",
    "        print(costs)\n",
    "        plt.plot(costs[0:last_iter_num])\n",
    "        plt.title(\"Costs\")\n",
    "        plt.show()\n",
    "\n",
    "        random_colors = np.random.random((n_clusters, 3))\n",
    "        colors = resp.dot(random_colors)    \n",
    "        plt.scatter(input_array[:,0], input_array[:,1], c=colors)\n",
    "        plt.show()\n",
    "\n",
    "    return centroids, resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7fb5dc6848a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_index_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# terms will go along rows, documents along columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "centroids, resp = plot_k_means(X, n_clusters=10, \n",
    "                 beta=1,\n",
    "                 max_iter=20, \n",
    "                 show_plots=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignment = resp.argmax(axis=1)\n",
    "\n",
    "len(cluster_assignment) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
